import os
from datetime import datetime
from urllib import parse

import boto3.session
import pandas as pd
import sagemaker
from airflow.decorators import dag, task
from airflow.hooks.base import BaseHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from imblearn.over_sampling import SMOTE
from include.dataprep.dataprep import preprocessing_pipeline
from sagemaker.sklearn.estimator import SKLearn

BUCKET_NAME = "mlops-zoomcamp-proj-mlflow-artifacts-bucket-20240724"


@dag(
    start_date=datetime(2024, 1, 1),
    schedule="@daily",
    catchup=False,
    tags=["mlops_zoomcamp"],
)
def training_pipeline():
    @task
    def get_training_data():
        s3_hook = S3Hook("my_aws_conn")
        file_name = s3_hook.download_file(
            "data/raw/train.csv",
            bucket_name=BUCKET_NAME,
            local_path="./data/",
            preserve_file_name=True,
            use_autogenerated_subdir=True,
        )
        print("Raw data path:", file_name)
        return file_name

    @task
    def prepare_data(raw_data_path: str):
        # Read raw data
        print("Reading CSV file")
        raw_data = pd.read_csv(raw_data_path, index_col="id")
        print("Raw data shape")
        print(raw_data.shape)
        print("Raw data shape:", raw_data.shape)
        # Apply data preprocessing
        print("Applying transformations")
        X = preprocessing_pipeline.fit_transform(raw_data.drop("Response", axis=1))
        y = raw_data["Response"]
        print("X shape:", X.shape)
        print("y shape:", y.shape)
        # Resample data
        print("Resampling data")
        smo = SMOTE()
        X_res, y_res = smo.fit_resample(X, y)
        print("X_res shape:", X_res.shape)
        print("y_res shape", y_res.shape)

        # Format training data
        print("Loading data to disk")
        training_data = pd.DataFrame(y_res).join(X_res)
        training_data_file_path = raw_data_path.replace(
            "train.csv", "prepared_data.csv"
        )
        training_data.to_csv(training_data_file_path, header=False, index=False)
        print("Training data file path:", training_data_file_path)
        print("training data shape:", training_data.shape)
        return training_data_file_path

    @task
    def upload_training_data_to_s3(training_data_local_path: str):
        # Read AWS connection info
        aws_conn = BaseHook.get_connection("my_aws_conn")
        # get boto3 session
        boto3_session = boto3.session.Session(
            aws_access_key_id=aws_conn.login,
            aws_secret_access_key=aws_conn.password,
            region_name="us-east-1",
        )
        sagemaker_session = sagemaker.Session(boto_session=boto3_session)

        tags = {"SageMaker": "true"}

        train_input = sagemaker_session.upload_data(
            path=training_data_local_path,
            bucket=BUCKET_NAME,
            key_prefix="data/{}".format(training_data_local_path.split("/")[1]),
            extra_args={"Tagging": parse.urlencode(tags)},
        )
        return train_input

    @task
    def train_model(training_data_s3_path: str):
        print("list dir:")
        print(os.listdir())
        # Read AWS connection info
        aws_conn = BaseHook.get_connection("my_aws_conn")
        # get boto3 session
        boto3_session = boto3.session.Session(
            aws_access_key_id=aws_conn.login,
            aws_secret_access_key=aws_conn.password,
            region_name="us-east-1",
        )
        sagemaker_session = sagemaker.Session(boto_session=boto3_session)
        script_path = "./include/train.py"
        sklearn_model = SKLearn(
            entry_point=script_path,
            framework_version="1.2-1",
            instance_type="ml.c5.2xlarge",
            sagemaker_session=sagemaker_session,
            role="arn:aws:iam::867991991201:role/test-role-for-sagemaker",
        )
        sklearn_model.fit({"train": training_data_s3_path})
        print(sklearn_model)
        print(vars(sklearn_model))
        return (
            sklearn_model.model_data,
            sklearn_model.role,
            sklearn_model.entry_point,
            sklearn_model.framework_version,
        )

    train_model(upload_training_data_to_s3(prepare_data(get_training_data())))


training_pipeline()
